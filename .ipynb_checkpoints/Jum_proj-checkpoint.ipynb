{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cInmTEWmpZTZ"
   },
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "### Importing the libraries needed for building the model.\n",
    "  - numpy to do some numerical operations\n",
    "  - pandas for cleaning and other processings needed by the data\n",
    "  - matplotlib for visualizing if there's a need to\n",
    "  - re[regular expressions for doing some cleaning on the text data itself:\n",
    "    - to remove some prefix and suffixes\n",
    "    - to remove emoji like comments\n",
    "    - to remove @ keyword etc\n",
    "  - bs4 to scrape the the website for the slangs used by Nigerians\n",
    "  - requests needed for the url of websites needed for scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nmqm5CySJ3bI"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8CmJk-LqZ7X"
   },
   "source": [
    "# Scraping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "768o4VvUKFd2"
   },
   "outputs": [],
   "source": [
    "url = \"https://insight.ng/spice/nigerian-slangs-dictionary/\"\n",
    "req = requests.get(url)\n",
    "soup = bs(req.text, \"html.parser\")\n",
    "slangs = soup.find(\"ul\", attrs = {\"class\": \"ez-toc-list-level-3\"} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXbUmwXyqemJ"
   },
   "source": [
    "# Creating a list for slang from the first website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-oKNUIJKTA2",
    "outputId": "ad64cd34-cdc9-4174-b34c-7ca4a5613e48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_list = []\n",
    "for slang in slangs.select('a'):\n",
    "  slang_list.append(slang.text)\n",
    "# print(slang_list)\n",
    "len(slang_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xokOZ5AnoKor"
   },
   "outputs": [],
   "source": [
    "new_slang_list = []\n",
    "for word in slang_list:\n",
    "  word = word.split('/')\n",
    "  new_slang_list.extend(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EFhdx2xbqUq3"
   },
   "outputs": [],
   "source": [
    "cleaned_slang_list_1 = []\n",
    "for word in new_slang_list:\n",
    "  word = word.replace('\\xa0', '')\n",
    "  word =  word.replace('.','')\n",
    "  word = word.replace(':','')\n",
    "  word = re.sub(\"\\d+\", '',word)\n",
    "  word = word.strip()\n",
    "  cleaned_slang_list_1.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPJh9BloqoNV"
   },
   "source": [
    "# Cleaning the texts gotten from the website for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "slGeD9-nrPjx"
   },
   "outputs": [],
   "source": [
    "unknown = ['About Author','Latest entries','+']\n",
    "for word in cleaned_slang_list_1:\n",
    "  if word in unknown:\n",
    "    cleaned_slang_list_1.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xifScz4oqxOh"
   },
   "source": [
    "### 58 Texts gotten from the first website after scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaIoVNwfwUpk",
    "outputId": "e423ed12-a881-4756-a35e-3e348eace67b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_slang_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6WKNhuB8II_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKM8660Tq3d8"
   },
   "source": [
    "# Scraping process for the second website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sz3IzxPUC_bC"
   },
   "outputs": [],
   "source": [
    "slangs_2_list = []\n",
    "\n",
    "url_2 = \"https://www.skabash.com/popular-nigerian-slangs-and-their-meanings/\"\n",
    "req_2 = requests.get(url_2)\n",
    "soup_2 = bs(req_2.text, \"html.parser\")\n",
    "slangs_2 = soup_2.find(\"div\", attrs={\"class\":\"lwptoc_items lwptoc_items-visible\"})\n",
    "\n",
    "for element in slangs_2.find(\"div\").select(\"div\"):\n",
    "  slangs_2_list.append(element.find('a').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6Rt1ma44DF9C"
   },
   "outputs": [],
   "source": [
    "new_slang_list_2 = []\n",
    "\n",
    "for slang in slangs_2_list:\n",
    "  slang = slang.replace('\\n', '')\n",
    "  slang = re.sub('\\d','',slang)\n",
    "  slang = slang.replace('.', '')\n",
    "  new_slang_list_2.append(slang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P99f4dY5DIXJ"
   },
   "outputs": [],
   "source": [
    "new_slang_list_2.remove(\"Popular Nigerian slangs that are trending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "y8pf_heULLCE"
   },
   "outputs": [],
   "source": [
    "new_slang_list_2 = list(set(new_slang_list_2))\n",
    "\n",
    "cleaned_slang_list_2 = []\n",
    "for word in new_slang_list_2:\n",
    "  word = word.split('/')\n",
    "  cleaned_slang_list_2.extend(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EiPzi0qq835"
   },
   "source": [
    "### 33 slangs gotten from the second website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stG106VDMRuC",
    "outputId": "f4f6b8c9-e308-4844-856d-7636e2ab0db8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_slang_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG-fXv0drCFj"
   },
   "source": [
    "### Combining the two slang lists into a single list by mere list addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbiAxgCEMz7Z",
    "outputId": "7e50ae1a-7ab7-48a2-969a-4b451729c213"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_list = cleaned_slang_list_1 + cleaned_slang_list_2\n",
    "len(long_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRePpJ4ErIAo"
   },
   "source": [
    "# SCRAPING THE **TWITTER** WEB IN SEARCH OF THE TWEETS THAT CONTAINS THE ABOVE WORDS USING __SNSCRAPE__ LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwC5FgidQji_",
    "outputId": "69ffa23c-7910-454b-ce7a-f786574a1353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (0.6.2.20230320)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from snscrape) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from snscrape) (4.11.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from snscrape) (4.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from snscrape) (3.6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\oyeni\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BdUiLgOKM08o"
   },
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as snstwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEcrGAlGrZZ6"
   },
   "source": [
    "# THE SCRAPING PROCESS\n",
    "\n",
    "  - The list was gotten by specifying the country's location via the coordinates as shown with the variable loc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lA7AedXfPRvY",
    "outputId": "183c448b-f946-4e16-c444-d64405cbdde4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oyeni\\AppData\\Local\\Temp\\ipykernel_23100\\1593414067.py:7: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  tweet_list.append([item.content, item.likeCount, item.user.location])\n"
     ]
    }
   ],
   "source": [
    "loc = '9.077751, 8.6774567, 100km'\n",
    "tweet_list = []\n",
    "for word in long_list:\n",
    "  for i, item in enumerate(snstwitter.TwitterSearchScraper('{} geocode:\"{}\"'.format(word, loc)).get_items()):\n",
    "    if i > 150:\n",
    "      break\n",
    "    tweet_list.append([item.content, item.likeCount, item.user.location])\n",
    "df = pd.DataFrame(tweet_list, columns = [\"tweets\", \"likes\", \"location\"])\n",
    "df.to_csv(\"slang_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = '9.077751, 8.6774567, 100km'\n",
    "tweet_list = []\n",
    "for word in short_list:\n",
    "  for i, item in enumerate(snstwitter.TwitterSearchScraper('{} geocode:\"{}\"'.format(word, loc)).get_items()):\n",
    "    if i > 150:\n",
    "      break\n",
    "    tweet_list.append([item.content, item.likeCount, item.user.location])\n",
    "df_short = pd.DataFrame(tweet_list, columns = [\"tweets\", \"likes\", \"location\"])\n",
    "df_short.to_csv(\"slang_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zh9RHE6qXWQQ",
    "outputId": "3eafc259-bc26-43c7-9c4f-ff13279c06b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_short.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short['label'] = 'not vulgar'\n",
    "df['label'] = 'vulgar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.concat([df, df_short], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seYVln8U2qSM"
   },
   "source": [
    "### Making a copy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "GPLIvZ762o9J",
    "outputId": "71999994-de3f-4689-eb5a-22571ef5505c"
   },
   "outputs": [],
   "source": [
    "df_1 = df_total.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IH7lEQ_mouR8"
   },
   "source": [
    "# **CLEANING THE TEXT DATA**\n",
    "### The Text data needs to be cleaned before being fed into the model for training and testing.\n",
    "    - Normalizing \n",
    "    - Remove Unicode Characters\n",
    "    - Remove Stopwords\n",
    "    - Perform Stemming\n",
    "    - Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoO5jYXboXC9"
   },
   "source": [
    "**Normalizing the texts..**\n",
    "  - changing all to lowercases also known as _case normalization_\n",
    "    - creating a function to used for changing the case to a lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7CFStpCoeXA"
   },
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "  text = text.lower()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBXtyATpssxe"
   },
   "source": [
    "**Removing Unicode Characters**\n",
    "  - Creating a function to remove unicode characters\n",
    "    - This function uses regular expression library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hS38jjfHsrDi"
   },
   "outputs": [],
   "source": [
    "def unicode_removal(text):\n",
    "  text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmOHc9EGtgpy"
   },
   "source": [
    "# **STOP WORDS**\n",
    "#### Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document. Generally, the most common words used in a text are “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7anwURsAtFp3"
   },
   "source": [
    "**Removing Stopwords**\n",
    "  - Creating a function to remove the stop words\n",
    "    - This function uses the Natural Language ToolKit library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDio_bL3tEOG"
   },
   "outputs": [],
   "source": [
    "# downloading the stopwords needed\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def stopwords_removal(text):\n",
    "  stop = stopwords.words('english')\n",
    "  text = \" \".join([word for word in text.split() if word not in (stop)])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQVXP4r3vNYI"
   },
   "source": [
    "# Stemming\n",
    "\n",
    "#### Stemming nvolves grouping of words by their root stem. This makes it clear or helps recognize that ‘jumping’ ‘jumps’ and ‘jumped’ are all rooted to the same verb (jump) and thus are referring to similar problems.\n",
    "  - Creating a function to stem the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le-mg_VcuXWU"
   },
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "  stemmer = PorterStemmer()\n",
    "  text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4TT60BEvvu5"
   },
   "source": [
    "# Lemmatization\n",
    "\n",
    "#### Lemmatization groups words based on root definition, and helps to differentiate between present, past, and indefinite.\n",
    "\n",
    "#### In order words, ‘jumps’ and ‘jump’ are grouped into the present ‘jump’, as different from all uses of ‘jumped’ which are grouped together as past tense, and all instances of ‘jumping’ which are grouped together as the indefinite (meaning continuing/continuous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2l-LIhFwDpB",
    "outputId": "504f163d-d386-4617-df3e-514e25c28271"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "def lemmatize(text):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy9zYvVa2d6L"
   },
   "source": [
    "### Selecting the useful feature we need to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCuNsvVu2kO9"
   },
   "outputs": [],
   "source": [
    "x = df_1['tweets']\n",
    "y = df_1['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aM1K7fz1i96"
   },
   "source": [
    "### Passing the text one by one through each of the functions created above using the lambda function method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XYqiBGBxPqS"
   },
   "outputs": [],
   "source": [
    "x = x.apply(lambda x: lowercase(x))\n",
    "x = x.apply(lambda x: unicode_removal(x))\n",
    "x = x.apply(lambda x: stopwords_removal(x))\n",
    "x = x.apply(lambda x: stemming(x))\n",
    "x = x.apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.apply(lambda y:1 if y == 'vulgar' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ni1Afut2agL"
   },
   "source": [
    "### Splitting the dataset into training and testing sets using the train test split..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yi_bVMA2xa2c"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test =  train_test_split(x, y, test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Count vectorizer to vectorize the preprocessed texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer =  CountVectorizer()\n",
    "x_train_vect = vectorizer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(x_train_vect.toarray(),y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vect = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_vect.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the accuracy score\n",
    "###    - f1 score\n",
    "###    - precision\n",
    "###    - recall\n",
    "###    - and the _confusion matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('classification reports:\\n',classification_report(y_test,model.predict(x_test_vect.toarray())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The model has 78% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving our decision tree model using pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('tweet.pkl','wb') as myfile:\n",
    "    pickle.dump(model,myfile)\n",
    "\n",
    "with open('tweet.pkl','rb') as myfile:\n",
    "    model = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
